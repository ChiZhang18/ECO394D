Hypothesis testing  
===================
date: James Scott (UT-Austin)
autosize: true
font-family: 'Gill Sans'
transition: none


<style>
.small-code pre code {
  font-size: 1em;
}
</style>

```{r setup, echo=FALSE, display=FALSE}
opts_chunk$set(cache=TRUE)
```


Reference: "Data Science" Chapter 7  


Outline
=====

- An introductory example  
- The four steps of every hypothesis test  
- $p$-values  
- permutation tests  


The Patriots
=======

Unless you're from a narrow strip of land from Connecticut to Maine, you probably dislike the New England Patriots.  

```{r, out.width = "400px", fig.align='center', echo=FALSE}
knitr::include_graphics("fig/patriots_logo.png") 
```


The Patriots
=======

First of all, they win too much.  

```{r, out.width = "650px", fig.align='center', echo=FALSE}
knitr::include_graphics("fig/patriots_superbowl.jpg") 
```




The Patriots
=======

Then there's Tom Brady, their star quarterback... 

```{r, out.width = "250px", fig.align='center', echo=FALSE}
knitr::include_graphics("fig/tb12.jpeg") 

```


"The more hydrated I am, the less likely I am to get sunburned."  --TB12  


The Patriots
=======

And Bill Belichick, their coach...

```{r, out.width = "650px", fig.align='center', echo=FALSE}
knitr::include_graphics("fig/EmperorBelichick.jpg") 
```


The Patriots
=======

And of course, the cheating!  

```{r, out.width = "500px", fig.align='center', echo=FALSE}
knitr::include_graphics("fig/gameball.jpg") 
```


The Patriots
=======
incremental: true 

But could even the Patriots cheat at the _pre-game coin toss?_  

Many people think so!  


The Patriots
=======


```{r, out.width = "550px", fig.align='center', echo=FALSE}
knitr::include_graphics("fig/coin_toss.png") 
```


The Patriots
=======
incremental: true

For a 25-game stretch during the 2014 and 2015 NFL seasons, the Patriots won the pre-game coin toss 19 out of 25 times, for a suspiciously high winning percentage of 76%.  

Shannon Sharpe: "This proves that either God or the devil is a Patriots fan, and it can't possibly be God."  


The Patriots
=======

"Use the Force..."

```{r, out.width = "500px", fig.align='center', echo=FALSE}
knitr::include_graphics("fig/EmperorBelichick.jpg") 
```



The Patriots
=======

But before we invoke religion or the Force to explain this fact, let's consider the innocent explanation first: blind luck.  
- If you toss a coin over and over again, you'll see some long streaks with more heads, and some with more tails, just by luck.  
- Is it plausible that the Patriots just went on a lucky 25-game streak?  


The Patriots
=======

To the code in `patriots.R`!  Let's simulate some coin flips.  


Summary
=======

This simple example has all the major elements of _hypothesis testing_:  
  1. We have a _null hypothesis_, that the pre-game coin toss in the Patriots' games was truly random.  
  2. We use a _test statistic_, number of Patriots' coin-toss wins, to measure the evidence against the null hypothesis.  
  3. We calculated the probability distribution of the test statistic, assuming that the null hypothesis is true.  Here, we just ran a Monte Carlo simulation of coin flips, assuming an unbiased coin.  
  4. Finally, we used this probability distribution to assess whether the null hypothesis looked believable in light of the data.  




Summary
=======

All hypothesis testing problems have these same four elements.  

  1. A null hypothesis $H_0$.  
  2. A test statistic $T \in \mathcal{T}$ that summarizes the data and measures the evidence against the null hypothesis.  (More extreme values of $T$ mean stronger evidence.)  
  3. $P(T \mid H_0)$:  the sampling distribution of the test statistic, assuming that the null hypothesis is true.  _This provides context for our measurement in step 2._  
  4. An assessment: in light of what we see in step 3, does our test statistic look plausible under the null hypothesis?  
  
  
  
Two schools of thought
=======

Within this basic framework, there are two schools of thought about how to proceed.  
  1. The Fisherian approach: step 4 is about __summarizing the evidence after the fact.__  Key terms: _$p$-value._   
  2. The Neyman-Pearson approach: step 4 is about __making a decision with ex-ante performance guarantees.__  Key terms: rejection region, $\alpha$ level, power curve.    

  
  
Fisher's approach
=======  

Suppose our observed test statistic is $t_{ob}$.  In step 4, we should report the quantity

$$
p = P(T \geq t_{ob} \mid H_0)
$$

Fisher call this the $p$-value: the probability that, if the null hypothesis were true, we would observe a test statistic $T$ at least as extreme as the value we actually observed ($t_{ob}$).  



For the Patriots' example  
=======

```{r, out.width = "750px", fig.align='center', echo=FALSE}
knitr::include_graphics("fig/patriots_coinflip.png") 
```



Fisher's approach
=======  

The $p$-value summarizes the strength of evidence provided by the data against the null hypothesis.
- $p$ closer to 0: data less likely under the null, so the null is more likely to be wrong.  __Stronger evidence against $H_0$.__
- $p$ further from 0: data more likely under the null.  __Weaker evidence against $H_0$.__

According to Fisher: job done!  _Report the $p$-value and let your readers make whatever they will of it._  


What do you mean by "close to 0"?  
=======  


What do you mean by "close to 0"?  
=======  

```{r, out.width = "350px", fig.align='center', echo=FALSE}
knitr::include_graphics("fig/fisher.jpg") 
```

***

"Mwa-ha-ha-ha-ha-ha!"

"You're on your own, suckahs!"  



p-values in the real world
=======  
incremental: true


The putative advantage of $p$-values is that they provide a sliding scale of evidence against the null hypothesis.  

The biggest problem with $p$-values is that they are nearly impossible to interpret.  


Problem 1: they are hard to interpret.
=======  

"I got a $p$-value of 0.02, so there's a 2% chance that the null hypothesis is right."

__Wrong__:
- $p = P(T \geq t_{ob} \mid H_0)$
- $p \neq P(H_0 \mid t_{ob})$  

Remember: conditional probabilities aren't symmetric!  


Problem 1: they are hard to interpret.
=======  

"I got a $p$-value of 0.02.  There's only a 2% chance I would have observed my test statistic if the null hypothesis were true."  

__Wrong__:
- $p = P(T \geq t_{ob} \mid H_0)$
- $p \neq P(T = t_{ob} \mid H_0)$  

Remember: the $p$-value is the probability of observing the test statistic you actually observed, __or any more extreme test statistic__, assuming $H_0$ is true.   



Problem 2: people oversimplify them  
=======  

Because $p$-values are hard to interpret, people tend to impose arbitrary cut-offs for what counts as a "significant" p-value.  

Psychologists, for example, will generally publish research findings for which $p < 0.05$.  


Problem 2: people oversimplify them  
=======  

Then again, psychologists will believe anything.

```{r, out.width = "850px", fig.align='center', echo=FALSE}
knitr::include_graphics("fig/powerpose.png") 
```



Problem 2: people oversimplify them  
=======  

Then again, psychologists will believe anything.

```{r, out.width = "550px", fig.align='center', echo=FALSE}
knitr::include_graphics("fig/himmicanes.png") 
```



Problem 2: people oversimplify them  
=======  

Then again, psychologists will believe anything.



```{r, out.width = "750px", fig.align='center', echo=FALSE}
knitr::include_graphics("fig/wear_red.png") 
```



Problem 2: people oversimplify them  
=======  
left: 40% 

Physicists are a bit more skeptical; they generally publish results when $p < 0.000001$.  

For example, the $p$-value in the paper announcing the discovery of the Higgs boson was $1.7 \times 10^{-9}$.  

***

```{r, out.width = "450px", fig.align='center', echo=FALSE}
knitr::include_graphics("fig/higgs.png") 
```



Neyman's criticisms of p-values  
=======  

1. Nobody except Fisher knows how to interpret them.   

2. Rejecting a null hypothesis isn't meaningful unless we have some alternative hypothesis in mind.  Since people will inevitably use a $p$-value to make a binary decision ("null" versus "alternative"), we should formalize that decision process.  



Neyman-Pearson testing   
=======

The Neyman-Pearson approach is aimed at quantifying (and controlling) the error probabilities associated with a hypothesis test.

False positive: rejecting $H_0$ when it is actually true.  ("Type I error")  

False negative: retaining $H_0$ when it is actually false.  ("Type II error")



Neyman-Pearson testing   
=======

In Neyman Pearson testing, we have a modified sequence of steps:  
  1. Specify $H_0$ __and $H_A$, an alternative hypothesis.__
  2. Choose your test statistic $T \in \mathcal{T}$.  
  3. calculate $P(T \mid H_0)$ __and $P(T \mid H_A)$.__  


Neyman-Pearson testing   
=======

_Before looking at the observed test statistic $t_{ob}$ for your actual data,_ continue as follows.

  4a. Specify a rejection region $R \subset \mathcal{T}$.    
  4b. Calculate $\alpha = P(T \in R \mid H_0)$.  This is called the alpha level or _size_ of the test.  
  4c. Calculate the _power_ of your test as $P(T \in R \mid H_A)$.  
  4d. Check whether your observed test statistic, $t_{ob}$, falls in $R$.  If so, reject $H_0$ in favor of $H_A$.  If not, retain $H_0$.  


Neyman-Pearson testing   
=======

The test is characterized by two properties:
- $\alpha = P(T \in R \mid H_0)$: the probability of falsely rejecting the null hypothesis when it's true.  
- $\beta = 1 -$ Power $= 1- P(T \in R \mid H_A)$: the probability of failing to reject the null hypothesis when it's false.  



Neyman-Pearson testing   
=======

At the end of a Neyman-Pearson test, you report:  
- the size ($\alpha$ level) of the test.  
- the _power_ of the test, or equivalently $\beta =  1 -$ power.  
- the result of the test: reject or retain ("fail to reject") the null hypothesis.  

No $p$-values!  


Neyman-Pearson testing   
=======

The difficulty of conducting a Neyman-Pearson test depends upon the alternative hypothesis.  
- "Simple" alternatives are easy: the power is just a single number.  
- "Composite" alternatives are a bit harder: the power is a function of an unknown parameter.  


Simple alternative
=======

Let's go back to the Patriots problem.  Our test statistic is $X$, the number of successful coin flips in 25 tries.  Suppose that $p$ is the true probability that the Patriots will win the coin toss.  Considering testing the two hypotheses:  
- $H_0: p = 1/2$.  
- $H_A: p = 2/3$.  

Suppose we decide to reject $H_0$ if $X \geq 17$.  In this case the power is easy to calculate: it's just $P(X \geq 17)$ when $X \sim \mbox{Binom}(N=25, p=2/3)$.  

Let's look at `power.R.`    


Composite alternative
=======

Compare this to the more realistic situation where our alternative hypothesis isn't so specific:  
- $H_0: p = 1/2$.  
- $H_A: p \neq 1/2$.

This is called a "composite alternative hypothesis."  _It "hedges its bets,__ i.e. it doesn't make any specific predictions except that the null hypothesis is wrong!  

Composite alternative
=======

Now the power of the test isn't just a single number.

Rather, it's a function, or a __power curve__:

$$
\mbox{Power}(p) = P(X \geq 17 \mid  p)
$$

where $p$ is the assumed binomial success probability.  This is a function of $p$.

Back to `power.R`.  